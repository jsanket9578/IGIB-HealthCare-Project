{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCLS_Healthcare\n",
    "\n",
    "### Team Members: \n",
    "* Sanket Jain [sanket.jain@plaksha.org]\n",
    "* Vinay Neekhra [vinay.neekhra@plaksha.org]\n",
    "* Nikita Arora [nikita.arora@plaksha.org]\n",
    "* Neha Gupta [neha.gupta@plaksha.org]\n",
    "* Susmeet Jain [susmeet.jain@plaksha.org]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Aim: To predict \"Gestational Age\" of 3 women based on the 7 multi-omics high-dimensional datasets. Training data consist of 14 women.\n",
    "More details can be found here: https://github.com/rintukutum/challengeLab-ML\n",
    "      \n",
    "<img src= \"https://raw.githubusercontent.com/rintukutum/challengeLab-ML/master/figures/figure-01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Results\n",
    "\n",
    "1. Number of featres much more than number of samples so simple models like LinearRegression and RandomForests tried.\n",
    "2. For some records, Gestational Age is negative which is difficult for the model to predict. Because of lack of domain knowledge, it is considered as is.\n",
    "3. Some datasets had more than 50000 features. Feature selection was performed for such cases, removing features on the basis of variance and correlation, and selecting top 100 features. The results didn't degrade by much, signifying that majority of the features are irrelevant and exhibit low correlation with the target.\n",
    "4. Mean Absolute Error comes out to be 3.2, 2.9, 1.4 years for the 3 parts respectively.\n",
    "5. This reveals that individually datasets in parts 1 and 2 don't perform that well and it is the combination of both of these that leads to good results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "comp_path = os.path.normpath(pathlib.Path().absolute())+os.path.normpath(\"/challengeLab-ML/data/train/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['immunome.csv',\n",
       " 'SerumLuminex.csv',\n",
       " 'plasmaLuminex.csv',\n",
       " 'plasmaSomalogic.csv']"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sub_cl1_file_names = \"immunome,SerumLuminex,plasmaLuminex,plasmaSomalogic\".split(\",\")\n",
    "sub_cl1_file_names = [x+\".csv\" for x in sub_cl1_file_names]\n",
    "sub_cl1_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immunome = pd.read_csv(os.path.join(comp_path,sub_cl1_file_names[0])) #68*535\n",
    "df_SerumLuminex = pd.read_csv(os.path.join(comp_path,sub_cl1_file_names[1]))\n",
    "df_plasmaLuminex = pd.read_csv(os.path.join(comp_path,sub_cl1_file_names[2]))\n",
    "df_plasmaSomalogic = pd.read_csv(os.path.join(comp_path,sub_cl1_file_names[3])) #1301 features!\n",
    "\n",
    "df_immunome[\"SampleID\"].unique().shape  #File has unique SampleID\n",
    "\n",
    "df_immunome = df_immunome.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "df_SerumLuminex = df_SerumLuminex.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "df_plasmaLuminex = df_plasmaLuminex.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "df_plasmaSomalogic = df_plasmaSomalogic.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl1 = pd.concat([df_immunome,df_SerumLuminex.iloc[:,1:],df_plasmaLuminex.iloc[:,1:],df_plasmaSomalogic.iloc[:,1:]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(pd.isna(df_cl1))\n",
    "#No Null values in any file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immunome.dtypes[1:].unique() #All columns float except the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd \"challengeLab-ML/data/\"\n",
    "targets_df = pd.read_csv(\"challenge-meta-information.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.,  4.,  1., 16., 16.,  1., 11.,  6.]),\n",
       " array([-3.42857143,  1.        ,  5.42857143,  9.85714286, 14.28571429,\n",
       "        18.71428571, 23.14285714, 27.57142857, 32.        ]),\n",
       " <a list of 8 Patch objects>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(targets_df[\"GA\"],bins=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, significant proportion of the dataset has negative gestational age which is inaccurate. Since we are not removing any rows from the dataset, these records will result in some error in the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df[\"randPerson\"].unique().shape\n",
    "#Total 17 women, each observed 4 times\n",
    "\n",
    "targets_df = targets_df[[\"GA\",\"SampleID\",\"data\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl1 = pd.merge(df_cl1,targets_df,on=\"SampleID\",).iloc[:,1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl1_train = df_cl1[df_cl1[:,-1]==\"train\"][:,:-1] \n",
    "df_cl1_test = df_cl1[df_cl1[:,-1]==\"test\"][:,:-1]\n",
    "#536 features, 1 label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_transf = MinMaxScaler()\n",
    "X_train = scaling_transf.fit_transform(df_cl1_train[:,:-1])\n",
    "X_test = scaling_transf.transform(df_cl1_test[:,:-1])\n",
    "\n",
    "y_train = df_cl1_train[:,-1]\n",
    "y_test = df_cl1_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.82, NNZs: 1858, Bias: 0.045709, T: 56, Avg. loss: 8.580110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.11, NNZs: 1776, Bias: 0.059104, T: 112, Avg. loss: 8.272012\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.18, NNZs: 1622, Bias: 0.053768, T: 168, Avg. loss: 7.761122\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.37, NNZs: 1565, Bias: 0.059246, T: 224, Avg. loss: 7.601926\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.53, NNZs: 1511, Bias: 0.064318, T: 280, Avg. loss: 7.272079\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.73, NNZs: 1498, Bias: 0.073706, T: 336, Avg. loss: 6.931575\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.80, NNZs: 1429, Bias: 0.069052, T: 392, Avg. loss: 6.863184\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.99, NNZs: 1437, Bias: 0.077748, T: 448, Avg. loss: 6.601004\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.05, NNZs: 1361, Bias: 0.069157, T: 504, Avg. loss: 6.394029\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.22, NNZs: 1370, Bias: 0.077411, T: 560, Avg. loss: 6.244379\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.29, NNZs: 1311, Bias: 0.069362, T: 616, Avg. loss: 6.101539\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.48, NNZs: 1338, Bias: 0.081183, T: 672, Avg. loss: 5.952262\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.54, NNZs: 1299, Bias: 0.073447, T: 728, Avg. loss: 5.755185\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.66, NNZs: 1300, Bias: 0.077288, T: 784, Avg. loss: 5.569163\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.77, NNZs: 1298, Bias: 0.081018, T: 840, Avg. loss: 5.459031\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.84, NNZs: 1283, Bias: 0.077309, T: 896, Avg. loss: 5.338010\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.92, NNZs: 1248, Bias: 0.073772, T: 952, Avg. loss: 5.219324\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 3.04, NNZs: 1273, Bias: 0.084485, T: 1008, Avg. loss: 5.280656\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 3.10, NNZs: 1258, Bias: 0.080966, T: 1064, Avg. loss: 5.120432\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 3.19, NNZs: 1267, Bias: 0.084460, T: 1120, Avg. loss: 5.051151\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 3.27, NNZs: 1253, Bias: 0.084448, T: 1176, Avg. loss: 4.892693\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 3.38, NNZs: 1262, Bias: 0.091201, T: 1232, Avg. loss: 4.778855\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 3.43, NNZs: 1250, Bias: 0.087820, T: 1288, Avg. loss: 4.790250\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 3.51, NNZs: 1248, Bias: 0.091139, T: 1344, Avg. loss: 4.680143\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 3.57, NNZs: 1256, Bias: 0.091117, T: 1400, Avg. loss: 4.666399\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 3.63, NNZs: 1247, Bias: 0.091112, T: 1456, Avg. loss: 4.557550\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 3.70, NNZs: 1233, Bias: 0.094332, T: 1512, Avg. loss: 4.534233\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 3.76, NNZs: 1227, Bias: 0.091134, T: 1568, Avg. loss: 4.499249\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 3.82, NNZs: 1223, Bias: 0.094284, T: 1624, Avg. loss: 4.391946\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 3.89, NNZs: 1208, Bias: 0.097411, T: 1680, Avg. loss: 4.373014\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 3.96, NNZs: 1210, Bias: 0.097405, T: 1736, Avg. loss: 4.332713\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 4.02, NNZs: 1217, Bias: 0.097400, T: 1792, Avg. loss: 4.207340\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 4.08, NNZs: 1207, Bias: 0.097397, T: 1848, Avg. loss: 4.176242\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 4.14, NNZs: 1203, Bias: 0.100422, T: 1904, Avg. loss: 4.161604\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 4.21, NNZs: 1195, Bias: 0.100413, T: 1960, Avg. loss: 4.110184\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 4.23, NNZs: 1183, Bias: 0.097419, T: 2016, Avg. loss: 4.046184\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 4.28, NNZs: 1176, Bias: 0.100389, T: 2072, Avg. loss: 4.052965\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 4.32, NNZs: 1171, Bias: 0.100387, T: 2128, Avg. loss: 4.011073\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 4.35, NNZs: 1160, Bias: 0.100395, T: 2184, Avg. loss: 3.968048\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 4.41, NNZs: 1152, Bias: 0.103307, T: 2240, Avg. loss: 3.956004\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 4.43, NNZs: 1146, Bias: 0.100403, T: 2296, Avg. loss: 3.946345\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 4.48, NNZs: 1140, Bias: 0.103291, T: 2352, Avg. loss: 3.875153\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 4.52, NNZs: 1138, Bias: 0.103296, T: 2408, Avg. loss: 3.898803\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 4.57, NNZs: 1136, Bias: 0.103296, T: 2464, Avg. loss: 3.865615\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 4.60, NNZs: 1130, Bias: 0.103299, T: 2520, Avg. loss: 3.854050\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 4.65, NNZs: 1131, Bias: 0.106116, T: 2576, Avg. loss: 3.809384\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 4.67, NNZs: 1125, Bias: 0.103328, T: 2632, Avg. loss: 3.766990\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 4.72, NNZs: 1126, Bias: 0.106115, T: 2688, Avg. loss: 3.767302\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 4.75, NNZs: 1122, Bias: 0.108880, T: 2744, Avg. loss: 3.761200\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 4.77, NNZs: 1118, Bias: 0.108860, T: 2800, Avg. loss: 3.691907\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 4.79, NNZs: 1111, Bias: 0.106120, T: 2856, Avg. loss: 3.720752\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 4.84, NNZs: 1111, Bias: 0.108851, T: 2912, Avg. loss: 3.766608\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 4.87, NNZs: 1106, Bias: 0.108854, T: 2968, Avg. loss: 3.700026\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 4.91, NNZs: 1101, Bias: 0.111560, T: 3024, Avg. loss: 3.660267\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 4.94, NNZs: 1100, Bias: 0.111556, T: 3080, Avg. loss: 3.614147\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 4.97, NNZs: 1096, Bias: 0.111552, T: 3136, Avg. loss: 3.637921\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 5.00, NNZs: 1094, Bias: 0.111558, T: 3192, Avg. loss: 3.640738\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 5.02, NNZs: 1087, Bias: 0.111560, T: 3248, Avg. loss: 3.590228\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 5.04, NNZs: 1080, Bias: 0.111568, T: 3304, Avg. loss: 3.584911\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 5.09, NNZs: 1088, Bias: 0.116830, T: 3360, Avg. loss: 3.616923\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 5.10, NNZs: 1071, Bias: 0.111589, T: 3416, Avg. loss: 3.545006\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 5.14, NNZs: 1078, Bias: 0.116812, T: 3472, Avg. loss: 3.556580\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 5.19, NNZs: 1077, Bias: 0.116807, T: 3528, Avg. loss: 3.538801\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 5.21, NNZs: 1071, Bias: 0.116808, T: 3584, Avg. loss: 3.530930\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 5.24, NNZs: 1072, Bias: 0.116805, T: 3640, Avg. loss: 3.518564\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 5.28, NNZs: 1069, Bias: 0.119372, T: 3696, Avg. loss: 3.411016\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 5.30, NNZs: 1062, Bias: 0.116810, T: 3752, Avg. loss: 3.471076\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 5.33, NNZs: 1056, Bias: 0.116816, T: 3808, Avg. loss: 3.432849\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 5.36, NNZs: 1055, Bias: 0.119358, T: 3864, Avg. loss: 3.457572\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 5.38, NNZs: 1054, Bias: 0.116831, T: 3920, Avg. loss: 3.413516\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 5.42, NNZs: 1053, Bias: 0.119357, T: 3976, Avg. loss: 3.398467\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 5.44, NNZs: 1049, Bias: 0.119359, T: 4032, Avg. loss: 3.368564\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 5.47, NNZs: 1040, Bias: 0.121865, T: 4088, Avg. loss: 3.372354\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 5.49, NNZs: 1037, Bias: 0.121863, T: 4144, Avg. loss: 3.341076\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 5.52, NNZs: 1033, Bias: 0.121860, T: 4200, Avg. loss: 3.376793\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 5.56, NNZs: 1029, Bias: 0.121860, T: 4256, Avg. loss: 3.281975\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 5.59, NNZs: 1030, Bias: 0.121860, T: 4312, Avg. loss: 3.287628\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 5.62, NNZs: 1028, Bias: 0.124322, T: 4368, Avg. loss: 3.331995\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 5.64, NNZs: 1026, Bias: 0.124319, T: 4424, Avg. loss: 3.293310\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 5.66, NNZs: 1024, Bias: 0.124322, T: 4480, Avg. loss: 3.220975\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 5.68, NNZs: 1020, Bias: 0.124322, T: 4536, Avg. loss: 3.267757\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 5.72, NNZs: 1023, Bias: 0.124324, T: 4592, Avg. loss: 3.249866\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 5.75, NNZs: 1021, Bias: 0.124325, T: 4648, Avg. loss: 3.241449\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 5.79, NNZs: 1019, Bias: 0.126743, T: 4704, Avg. loss: 3.241111\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 5.81, NNZs: 1017, Bias: 0.126739, T: 4760, Avg. loss: 3.247454\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 5.83, NNZs: 1012, Bias: 0.124340, T: 4816, Avg. loss: 3.177012\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 5.87, NNZs: 1010, Bias: 0.126742, T: 4872, Avg. loss: 3.159426\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 5.89, NNZs: 1005, Bias: 0.126743, T: 4928, Avg. loss: 3.168897\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 5.92, NNZs: 1006, Bias: 0.129127, T: 4984, Avg. loss: 3.189836\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 5.95, NNZs: 1007, Bias: 0.129123, T: 5040, Avg. loss: 3.163814\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 5.96, NNZs: 996, Bias: 0.129122, T: 5096, Avg. loss: 3.136552\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 5.98, NNZs: 997, Bias: 0.131485, T: 5152, Avg. loss: 3.107512\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 6.00, NNZs: 993, Bias: 0.129128, T: 5208, Avg. loss: 3.146456\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 6.04, NNZs: 993, Bias: 0.131479, T: 5264, Avg. loss: 3.087549\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 6.05, NNZs: 990, Bias: 0.129137, T: 5320, Avg. loss: 3.101093\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 6.08, NNZs: 984, Bias: 0.131478, T: 5376, Avg. loss: 3.058367\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 6.11, NNZs: 985, Bias: 0.133813, T: 5432, Avg. loss: 3.051360\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 6.13, NNZs: 982, Bias: 0.133810, T: 5488, Avg. loss: 3.071730\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 6.16, NNZs: 980, Bias: 0.133810, T: 5544, Avg. loss: 3.032676\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 6.19, NNZs: 980, Bias: 0.138433, T: 5600, Avg. loss: 3.029761\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 6.21, NNZs: 975, Bias: 0.136118, T: 5656, Avg. loss: 3.047937\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 6.22, NNZs: 964, Bias: 0.133816, T: 5712, Avg. loss: 3.027868\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 6.24, NNZs: 970, Bias: 0.133822, T: 5768, Avg. loss: 2.986877\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 6.26, NNZs: 967, Bias: 0.136115, T: 5824, Avg. loss: 2.992792\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 6.28, NNZs: 965, Bias: 0.136117, T: 5880, Avg. loss: 2.968134\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 6.32, NNZs: 959, Bias: 0.138396, T: 5936, Avg. loss: 2.994783\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 6.33, NNZs: 959, Bias: 0.136120, T: 5992, Avg. loss: 2.975428\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 6.36, NNZs: 958, Bias: 0.136123, T: 6048, Avg. loss: 2.956197\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 6.39, NNZs: 956, Bias: 0.138390, T: 6104, Avg. loss: 2.937634\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 6.41, NNZs: 959, Bias: 0.140649, T: 6160, Avg. loss: 2.947587\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 6.43, NNZs: 955, Bias: 0.138392, T: 6216, Avg. loss: 2.921922\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 6.46, NNZs: 955, Bias: 0.140641, T: 6272, Avg. loss: 2.898543\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 6.48, NNZs: 948, Bias: 0.140640, T: 6328, Avg. loss: 2.907940\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 6.50, NNZs: 946, Bias: 0.140640, T: 6384, Avg. loss: 2.884626\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 6.52, NNZs: 944, Bias: 0.140642, T: 6440, Avg. loss: 2.906046\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 6.55, NNZs: 944, Bias: 0.142871, T: 6496, Avg. loss: 2.884987\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 6.58, NNZs: 941, Bias: 0.142868, T: 6552, Avg. loss: 2.863739\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 6.59, NNZs: 941, Bias: 0.140648, T: 6608, Avg. loss: 2.865550\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 6.62, NNZs: 945, Bias: 0.142864, T: 6664, Avg. loss: 2.856142\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 6.63, NNZs: 938, Bias: 0.140656, T: 6720, Avg. loss: 2.812679\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 6.67, NNZs: 933, Bias: 0.142864, T: 6776, Avg. loss: 2.780915\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 6.69, NNZs: 932, Bias: 0.142866, T: 6832, Avg. loss: 2.826723\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 6.72, NNZs: 935, Bias: 0.145063, T: 6888, Avg. loss: 2.808699\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 6.72, NNZs: 928, Bias: 0.142871, T: 6944, Avg. loss: 2.806307\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 6.75, NNZs: 926, Bias: 0.145059, T: 7000, Avg. loss: 2.788704\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 6.78, NNZs: 933, Bias: 0.147241, T: 7056, Avg. loss: 2.773485\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 6.79, NNZs: 926, Bias: 0.145060, T: 7112, Avg. loss: 2.777619\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 6.81, NNZs: 920, Bias: 0.142888, T: 7168, Avg. loss: 2.738253\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 6.84, NNZs: 925, Bias: 0.147232, T: 7224, Avg. loss: 2.767426\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 6.86, NNZs: 923, Bias: 0.147233, T: 7280, Avg. loss: 2.751867\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 6.89, NNZs: 920, Bias: 0.149394, T: 7336, Avg. loss: 2.734632\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 6.90, NNZs: 910, Bias: 0.147234, T: 7392, Avg. loss: 2.716674\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 6.92, NNZs: 909, Bias: 0.147235, T: 7448, Avg. loss: 2.675488\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 6.94, NNZs: 914, Bias: 0.149387, T: 7504, Avg. loss: 2.708201\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 6.96, NNZs: 911, Bias: 0.147242, T: 7560, Avg. loss: 2.697611\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 6.98, NNZs: 910, Bias: 0.149387, T: 7616, Avg. loss: 2.686115\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 7.01, NNZs: 911, Bias: 0.151525, T: 7672, Avg. loss: 2.681928\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 7.03, NNZs: 906, Bias: 0.151526, T: 7728, Avg. loss: 2.648423\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 7.06, NNZs: 904, Bias: 0.151526, T: 7784, Avg. loss: 2.654397\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 7.08, NNZs: 900, Bias: 0.151526, T: 7840, Avg. loss: 2.659225\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 7.11, NNZs: 908, Bias: 0.153648, T: 7896, Avg. loss: 2.627775\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 7.13, NNZs: 904, Bias: 0.151527, T: 7952, Avg. loss: 2.618889\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 7.14, NNZs: 903, Bias: 0.151527, T: 8008, Avg. loss: 2.606535\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 7.16, NNZs: 898, Bias: 0.151528, T: 8064, Avg. loss: 2.637893\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 7.18, NNZs: 897, Bias: 0.151531, T: 8120, Avg. loss: 2.617824\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 7.20, NNZs: 895, Bias: 0.153636, T: 8176, Avg. loss: 2.587134\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 7.22, NNZs: 894, Bias: 0.153637, T: 8232, Avg. loss: 2.596397\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 7.24, NNZs: 894, Bias: 0.155733, T: 8288, Avg. loss: 2.592261\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 7.27, NNZs: 894, Bias: 0.153638, T: 8344, Avg. loss: 2.575199\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 7.29, NNZs: 892, Bias: 0.155728, T: 8400, Avg. loss: 2.569273\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 7.30, NNZs: 892, Bias: 0.155727, T: 8456, Avg. loss: 2.564556\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 7.32, NNZs: 894, Bias: 0.153644, T: 8512, Avg. loss: 2.556491\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 7.35, NNZs: 894, Bias: 0.155728, T: 8568, Avg. loss: 2.542373\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 7.37, NNZs: 891, Bias: 0.157802, T: 8624, Avg. loss: 2.518571\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 7.38, NNZs: 892, Bias: 0.155728, T: 8680, Avg. loss: 2.519783\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 7.40, NNZs: 889, Bias: 0.157798, T: 8736, Avg. loss: 2.517924\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 7.41, NNZs: 887, Bias: 0.153666, T: 8792, Avg. loss: 2.501367\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 7.44, NNZs: 886, Bias: 0.155733, T: 8848, Avg. loss: 2.522320\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 7.46, NNZs: 889, Bias: 0.157796, T: 8904, Avg. loss: 2.499701\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 7.48, NNZs: 888, Bias: 0.157797, T: 8960, Avg. loss: 2.487924\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 7.51, NNZs: 887, Bias: 0.159850, T: 9016, Avg. loss: 2.482158\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 7.52, NNZs: 887, Bias: 0.159849, T: 9072, Avg. loss: 2.480045\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 7.55, NNZs: 885, Bias: 0.159850, T: 9128, Avg. loss: 2.468506\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 7.57, NNZs: 884, Bias: 0.161895, T: 9184, Avg. loss: 2.459915\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 7.59, NNZs: 884, Bias: 0.161893, T: 9240, Avg. loss: 2.448618\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 7.61, NNZs: 885, Bias: 0.165966, T: 9296, Avg. loss: 2.427000\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 7.63, NNZs: 879, Bias: 0.163928, T: 9352, Avg. loss: 2.455502\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 7.64, NNZs: 871, Bias: 0.159864, T: 9408, Avg. loss: 2.406870\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 7.66, NNZs: 872, Bias: 0.161895, T: 9464, Avg. loss: 2.423544\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 7.68, NNZs: 873, Bias: 0.161896, T: 9520, Avg. loss: 2.418068\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 7.71, NNZs: 870, Bias: 0.163920, T: 9576, Avg. loss: 2.392031\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 7.72, NNZs: 864, Bias: 0.161901, T: 9632, Avg. loss: 2.394884\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 7.75, NNZs: 872, Bias: 0.165935, T: 9688, Avg. loss: 2.394723\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 7.76, NNZs: 866, Bias: 0.163919, T: 9744, Avg. loss: 2.384351\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 7.78, NNZs: 863, Bias: 0.165928, T: 9800, Avg. loss: 2.360644\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 7.80, NNZs: 862, Bias: 0.165926, T: 9856, Avg. loss: 2.367393\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 7.81, NNZs: 857, Bias: 0.163920, T: 9912, Avg. loss: 2.353006\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 7.83, NNZs: 857, Bias: 0.163923, T: 9968, Avg. loss: 2.343935\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 7.85, NNZs: 858, Bias: 0.165923, T: 10024, Avg. loss: 2.338561\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 7.87, NNZs: 856, Bias: 0.165924, T: 10080, Avg. loss: 2.347552\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 7.89, NNZs: 854, Bias: 0.167917, T: 10136, Avg. loss: 2.330634\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 7.90, NNZs: 853, Bias: 0.163933, T: 10192, Avg. loss: 2.313596\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 7.93, NNZs: 855, Bias: 0.165924, T: 10248, Avg. loss: 2.312955\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 7.95, NNZs: 855, Bias: 0.167910, T: 10304, Avg. loss: 2.317592\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 7.97, NNZs: 858, Bias: 0.165928, T: 10360, Avg. loss: 2.298090\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 7.99, NNZs: 854, Bias: 0.167910, T: 10416, Avg. loss: 2.304375\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 8.01, NNZs: 852, Bias: 0.167910, T: 10472, Avg. loss: 2.262650\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 8.03, NNZs: 854, Bias: 0.169884, T: 10528, Avg. loss: 2.278840\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 8.05, NNZs: 855, Bias: 0.169882, T: 10584, Avg. loss: 2.278112\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 8.07, NNZs: 852, Bias: 0.167910, T: 10640, Avg. loss: 2.239598\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 8.08, NNZs: 852, Bias: 0.169879, T: 10696, Avg. loss: 2.265195\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 8.10, NNZs: 850, Bias: 0.169879, T: 10752, Avg. loss: 2.245300\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 8.11, NNZs: 845, Bias: 0.167918, T: 10808, Avg. loss: 2.216026\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 8.13, NNZs: 848, Bias: 0.169880, T: 10864, Avg. loss: 2.246146\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 8.15, NNZs: 846, Bias: 0.169881, T: 10920, Avg. loss: 2.224460\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 8.17, NNZs: 847, Bias: 0.171836, T: 10976, Avg. loss: 2.215127\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 8.19, NNZs: 846, Bias: 0.171835, T: 11032, Avg. loss: 2.221741\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 8.21, NNZs: 845, Bias: 0.173784, T: 11088, Avg. loss: 2.202062\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 8.23, NNZs: 847, Bias: 0.171836, T: 11144, Avg. loss: 2.209176\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 8.24, NNZs: 847, Bias: 0.173781, T: 11200, Avg. loss: 2.181396\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 8.25, NNZs: 844, Bias: 0.173780, T: 11256, Avg. loss: 2.179335\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 8.27, NNZs: 844, Bias: 0.173781, T: 11312, Avg. loss: 2.180450\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 8.29, NNZs: 843, Bias: 0.173782, T: 11368, Avg. loss: 2.162371\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 8.31, NNZs: 840, Bias: 0.175718, T: 11424, Avg. loss: 2.153313\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 8.32, NNZs: 839, Bias: 0.173783, T: 11480, Avg. loss: 2.168447\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 8.34, NNZs: 839, Bias: 0.175714, T: 11536, Avg. loss: 2.148990\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 8.35, NNZs: 838, Bias: 0.173786, T: 11592, Avg. loss: 2.150730\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 8.37, NNZs: 838, Bias: 0.175713, T: 11648, Avg. loss: 2.141033\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 8.39, NNZs: 837, Bias: 0.175713, T: 11704, Avg. loss: 2.130570\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 8.41, NNZs: 837, Bias: 0.175714, T: 11760, Avg. loss: 2.126260\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 8.43, NNZs: 836, Bias: 0.177634, T: 11816, Avg. loss: 2.126231\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 8.45, NNZs: 838, Bias: 0.177634, T: 11872, Avg. loss: 2.106652\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 8.46, NNZs: 837, Bias: 0.177634, T: 11928, Avg. loss: 2.109291\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 8.47, NNZs: 835, Bias: 0.179547, T: 11984, Avg. loss: 2.098096\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 8.50, NNZs: 836, Bias: 0.179547, T: 12040, Avg. loss: 2.096178\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 8.51, NNZs: 836, Bias: 0.181453, T: 12096, Avg. loss: 2.071066\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 8.52, NNZs: 833, Bias: 0.177640, T: 12152, Avg. loss: 2.077510\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 8.53, NNZs: 832, Bias: 0.177642, T: 12208, Avg. loss: 2.052652\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 8.55, NNZs: 829, Bias: 0.179544, T: 12264, Avg. loss: 2.085757\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 8.57, NNZs: 830, Bias: 0.177646, T: 12320, Avg. loss: 2.074595\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 8.58, NNZs: 830, Bias: 0.179543, T: 12376, Avg. loss: 2.070557\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 8.60, NNZs: 827, Bias: 0.181438, T: 12432, Avg. loss: 2.066370\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 8.61, NNZs: 828, Bias: 0.181437, T: 12488, Avg. loss: 2.049443\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 8.62, NNZs: 828, Bias: 0.179546, T: 12544, Avg. loss: 2.042334\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 8.64, NNZs: 828, Bias: 0.181436, T: 12600, Avg. loss: 2.055142\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 8.65, NNZs: 825, Bias: 0.181436, T: 12656, Avg. loss: 2.034479\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 8.67, NNZs: 824, Bias: 0.181437, T: 12712, Avg. loss: 2.034463\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 8.68, NNZs: 823, Bias: 0.181438, T: 12768, Avg. loss: 2.042561\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 8.69, NNZs: 826, Bias: 0.183319, T: 12824, Avg. loss: 2.029871\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 8.71, NNZs: 823, Bias: 0.181442, T: 12880, Avg. loss: 2.024466\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 8.73, NNZs: 825, Bias: 0.185194, T: 12936, Avg. loss: 2.012521\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 8.73, NNZs: 826, Bias: 0.183319, T: 12992, Avg. loss: 2.025345\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 8.75, NNZs: 823, Bias: 0.183319, T: 13048, Avg. loss: 2.015129\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 8.75, NNZs: 824, Bias: 0.183320, T: 13104, Avg. loss: 2.017253\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 8.77, NNZs: 823, Bias: 0.185188, T: 13160, Avg. loss: 2.003959\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 8.79, NNZs: 826, Bias: 0.185188, T: 13216, Avg. loss: 2.005683\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 8.81, NNZs: 824, Bias: 0.185188, T: 13272, Avg. loss: 1.998229\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 8.82, NNZs: 824, Bias: 0.185188, T: 13328, Avg. loss: 1.976373\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 8.83, NNZs: 822, Bias: 0.185188, T: 13384, Avg. loss: 1.998551\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 8.85, NNZs: 822, Bias: 0.185188, T: 13440, Avg. loss: 1.982950\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 8.86, NNZs: 819, Bias: 0.185187, T: 13496, Avg. loss: 1.995936\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 8.87, NNZs: 818, Bias: 0.185188, T: 13552, Avg. loss: 1.962320\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 8.89, NNZs: 819, Bias: 0.185188, T: 13608, Avg. loss: 1.966460\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 8.90, NNZs: 820, Bias: 0.185188, T: 13664, Avg. loss: 1.970411\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 8.91, NNZs: 821, Bias: 0.185189, T: 13720, Avg. loss: 1.951018\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 8.93, NNZs: 820, Bias: 0.187036, T: 13776, Avg. loss: 1.960505\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 8.93, NNZs: 820, Bias: 0.187036, T: 13832, Avg. loss: 1.970181\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 8.94, NNZs: 821, Bias: 0.187035, T: 13888, Avg. loss: 1.973750\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 8.96, NNZs: 820, Bias: 0.187035, T: 13944, Avg. loss: 1.959425\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 8.98, NNZs: 821, Bias: 0.188874, T: 14000, Avg. loss: 1.955813\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 8.99, NNZs: 819, Bias: 0.188873, T: 14056, Avg. loss: 1.945003\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 9.00, NNZs: 816, Bias: 0.187037, T: 14112, Avg. loss: 1.951846\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 9.00, NNZs: 818, Bias: 0.187039, T: 14168, Avg. loss: 1.949558\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 9.02, NNZs: 814, Bias: 0.190704, T: 14224, Avg. loss: 1.951463\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 9.03, NNZs: 815, Bias: 0.188873, T: 14280, Avg. loss: 1.931697\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 9.05, NNZs: 812, Bias: 0.190702, T: 14336, Avg. loss: 1.935861\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 9.06, NNZs: 813, Bias: 0.188875, T: 14392, Avg. loss: 1.938660\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 9.07, NNZs: 812, Bias: 0.190701, T: 14448, Avg. loss: 1.918683\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 9.08, NNZs: 810, Bias: 0.190701, T: 14504, Avg. loss: 1.933726\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 9.09, NNZs: 813, Bias: 0.190701, T: 14560, Avg. loss: 1.927823\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 9.11, NNZs: 811, Bias: 0.190701, T: 14616, Avg. loss: 1.918534\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 9.12, NNZs: 812, Bias: 0.190702, T: 14672, Avg. loss: 1.903991\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 9.14, NNZs: 812, Bias: 0.192518, T: 14728, Avg. loss: 1.916119\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 9.15, NNZs: 810, Bias: 0.192517, T: 14784, Avg. loss: 1.909531\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 9.15, NNZs: 809, Bias: 0.190704, T: 14840, Avg. loss: 1.913709\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 9.17, NNZs: 807, Bias: 0.194326, T: 14896, Avg. loss: 1.895656\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 9.17, NNZs: 811, Bias: 0.192516, T: 14952, Avg. loss: 1.885064\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 9.19, NNZs: 809, Bias: 0.190709, T: 15008, Avg. loss: 1.903922\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 9.20, NNZs: 808, Bias: 0.192516, T: 15064, Avg. loss: 1.910329\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 9.22, NNZs: 808, Bias: 0.190713, T: 15120, Avg. loss: 1.888263\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 9.23, NNZs: 806, Bias: 0.192516, T: 15176, Avg. loss: 1.893806\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 9.23, NNZs: 809, Bias: 0.194317, T: 15232, Avg. loss: 1.885667\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 9.25, NNZs: 805, Bias: 0.194316, T: 15288, Avg. loss: 1.884569\n",
      "Total training time: 0.71 seconds.\n",
      "Convergence after 273 epochs took 0.71 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.01, average=False, early_stopping=False, epsilon=0,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='epsilon_insensitive',\n",
       "             max_iter=300, n_iter_no_change=6, penalty='l1', power_t=0.25,\n",
       "             random_state=None, shuffle=True, tol=0.001,\n",
       "             validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#using MAE - quantile regression\n",
    "regressor = SGDRegressor(loss=\"epsilon_insensitive\",epsilon=0,penalty=\"l1\",alpha=0.01,max_iter=300,eta0=0.01,power_t=0.25,learning_rate=\"invscaling\",n_iter_no_change=6,verbose=1)\n",
    "regressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.267423350516629"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=regressor.predict(X_test)\n",
    "mean_absolute_error(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression doesn't work well, MAE of 4.27 yrs means it's not a very good model. Let's try an ensemble of models (RandomForest) where we will utilize the concept of bagging to make multiple \"weak\" models that make uncorrelated errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=1,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#50 estimators, leaves contain less than 2 samples, division only when 1 sample in leaves, boostrap ON but all samples,\n",
    "#see only sqrt(features) \n",
    "\n",
    "rfRegressor = RandomForestRegressor(n_estimators=100,criterion=\"mae\",min_samples_split=2,min_samples_leaf=1,bootstrap=True,n_jobs=-1,verbose=1) #max_features=\"auto\"\n",
    "rfRegressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5884948979591835\n",
      "3.181547619047619\n"
     ]
    }
   ],
   "source": [
    "y_pred=rfRegressor.predict(X_train)\n",
    "\n",
    "print(mean_absolute_error(y_train,y_pred))\n",
    "\n",
    "\n",
    "y_pred=rfRegressor.predict(X_test)\n",
    "print(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE of 3.2 is significantly better than the earlier 4.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cfRNA.csv', 'metabolome.csv', 'microbiome.csv']"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_cl2_file_names = \"cfRNA,metabolome,microbiome\".split(\",\")\n",
    "sub_cl2_file_names = [x+\".csv\" for x in sub_cl2_file_names]\n",
    "sub_cl2_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cfRNA = pd.read_csv(os.path.join(comp_path,sub_cl2_file_names[0])) \n",
    "df_metabolome = pd.read_csv(os.path.join(comp_path,sub_cl2_file_names[1]),encoding=\"iso-8859-1\")\n",
    "df_microbiome = pd.read_csv(os.path.join(comp_path,sub_cl2_file_names[2]))\n",
    "\n",
    "\n",
    "df_cfRNA = df_cfRNA.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "df_metabolome = df_metabolome.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "df_microbiome = df_microbiome.sort_values(by=\"SampleID\",axis=0,kind=\"mergesort\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl2 = pd.concat([df_cfRNA,df_metabolome.iloc[:,1:],df_microbiome.iloc[:,1:]],axis=1)\n",
    "df_cl2 =pd.merge(df_cl2,targets_df,on=\"SampleID\",).iloc[:,1:] #.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SampleID</th>\n",
       "      <th>C2orf76</th>\n",
       "      <th>ACTL10</th>\n",
       "      <th>CEP135</th>\n",
       "      <th>RP11-613M10.6</th>\n",
       "      <th>NDUFB5P1</th>\n",
       "      <th>MIIP</th>\n",
       "      <th>RP11-98I9.4</th>\n",
       "      <th>C20orf144</th>\n",
       "      <th>RP11-485G7.6</th>\n",
       "      <th>...</th>\n",
       "      <th>ToothGum_Azospirillum</th>\n",
       "      <th>Stool_Azospirillum</th>\n",
       "      <th>VaginalSwab_Sphingomonas.5</th>\n",
       "      <th>Saliva_Sphingomonas.5</th>\n",
       "      <th>ToothGum_Sphingomonas.5</th>\n",
       "      <th>Stool_Sphingomonas.5</th>\n",
       "      <th>VaginalSwab_Thalassospira.9</th>\n",
       "      <th>Saliva_Thalassospira.9</th>\n",
       "      <th>ToothGum_Thalassospira.9</th>\n",
       "      <th>Stool_Thalassospira.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>B2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.022863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>-0.034891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>B3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.646714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "      <td>-0.025123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>B4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "      <td>-0.034209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G1</td>\n",
       "      <td>2.126079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.106183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.756472</td>\n",
       "      <td>1.063039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>-0.024750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>W4</td>\n",
       "      <td>35.254263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.204933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.614799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "      <td>-0.026713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>X1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.638411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.468877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>X2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.901872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.022884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>X3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "      <td>-0.026088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>X4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.068161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.068161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.068161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.023462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 59309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SampleID    C2orf76  ACTL10     CEP135  RP11-613M10.6  NDUFB5P1       MIIP  \\\n",
       "13       B1   0.000000     0.0   0.000000            0.0       0.0   0.000000   \n",
       "30       B2   0.000000     0.0  34.022863            0.0       0.0   0.000000   \n",
       "47       B3   0.000000     0.0  97.646714            0.0       0.0   0.000000   \n",
       "64       B4   0.000000     0.0   0.000000            0.0       0.0   0.000000   \n",
       "6        G1   2.126079     0.0  86.106183            0.0       0.0  12.756472   \n",
       "..      ...        ...     ...        ...            ...       ...        ...   \n",
       "63       W4  35.254263     0.0   3.204933            0.0       0.0   9.614799   \n",
       "10       X1   0.000000     0.0  10.638411            0.0       0.0  74.468877   \n",
       "27       X2   0.000000     0.0   0.000000            0.0       0.0  70.901872   \n",
       "44       X3   0.000000     0.0   0.000000            0.0       0.0   0.000000   \n",
       "61       X4   0.000000     0.0  10.068161            0.0       0.0  10.068161   \n",
       "\n",
       "    RP11-98I9.4  C20orf144  RP11-485G7.6  ...  ToothGum_Azospirillum  \\\n",
       "13     0.000000   0.000000           0.0  ...              -0.034209   \n",
       "30     0.000000   0.000000           0.0  ...              -0.034891   \n",
       "47     0.000000   0.000000           0.0  ...              -0.025123   \n",
       "64     0.000000   0.000000           0.0  ...              -0.034209   \n",
       "6      1.063039   0.000000           0.0  ...              -0.024750   \n",
       "..          ...        ...           ...  ...                    ...   \n",
       "63     0.000000   0.000000           0.0  ...              -0.026713   \n",
       "10     0.000000   0.000000           0.0  ...              -0.023462   \n",
       "27     0.000000   0.000000           0.0  ...              -0.022884   \n",
       "44     0.000000   0.000000           0.0  ...              -0.026088   \n",
       "61     0.000000  10.068161           0.0  ...              -0.023462   \n",
       "\n",
       "    Stool_Azospirillum  VaginalSwab_Sphingomonas.5  Saliva_Sphingomonas.5  \\\n",
       "13           -0.034209                   -0.034209              -0.034209   \n",
       "30           -0.034891                   -0.034891              -0.034891   \n",
       "47           -0.025123                   -0.025123              -0.025123   \n",
       "64           -0.034209                   -0.034209              -0.034209   \n",
       "6            -0.024750                   -0.024750              -0.024750   \n",
       "..                 ...                         ...                    ...   \n",
       "63           -0.026713                   -0.026713              -0.026713   \n",
       "10           -0.023462                   -0.023462              -0.023462   \n",
       "27           -0.022884                   -0.022884              -0.022884   \n",
       "44           -0.026088                   -0.026088              -0.026088   \n",
       "61           -0.023462                   -0.023462              -0.023462   \n",
       "\n",
       "    ToothGum_Sphingomonas.5  Stool_Sphingomonas.5  \\\n",
       "13                -0.034209             -0.034209   \n",
       "30                -0.034891             -0.034891   \n",
       "47                -0.025123             -0.025123   \n",
       "64                -0.034209             -0.034209   \n",
       "6                 -0.024750             -0.024750   \n",
       "..                      ...                   ...   \n",
       "63                -0.026713             -0.026713   \n",
       "10                -0.023462             -0.023462   \n",
       "27                -0.022884             -0.022884   \n",
       "44                -0.026088             -0.026088   \n",
       "61                -0.023462             -0.023462   \n",
       "\n",
       "    VaginalSwab_Thalassospira.9  Saliva_Thalassospira.9  \\\n",
       "13                    -0.034209               -0.034209   \n",
       "30                    -0.034891               -0.034891   \n",
       "47                    -0.025123               -0.025123   \n",
       "64                    -0.034209               -0.034209   \n",
       "6                     -0.024750               -0.024750   \n",
       "..                          ...                     ...   \n",
       "63                    -0.026713               -0.026713   \n",
       "10                    -0.023462               -0.023462   \n",
       "27                    -0.022884               -0.022884   \n",
       "44                    -0.026088               -0.026088   \n",
       "61                    -0.023462               -0.023462   \n",
       "\n",
       "    ToothGum_Thalassospira.9  Stool_Thalassospira.9  \n",
       "13                 -0.034209              -0.034209  \n",
       "30                 -0.034891              -0.034891  \n",
       "47                 -0.025123              -0.025123  \n",
       "64                 -0.034209              -0.034209  \n",
       "6                  -0.024750              -0.024750  \n",
       "..                       ...                    ...  \n",
       "63                 -0.026713              -0.026713  \n",
       "10                 -0.023462              -0.023462  \n",
       "27                 -0.022884              -0.022884  \n",
       "44                 -0.026088              -0.026088  \n",
       "61                 -0.023462              -0.023462  \n",
       "\n",
       "[68 rows x 59309 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl2\n",
    "#59308 features now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl2_train = df_cl2[df_cl2.iloc[:,-1]==\"train\"].iloc[:,:-1] \n",
    "df_cl2_test = df_cl2[df_cl2.iloc[:,-1]==\"test\"].iloc[:,:-1]\n",
    "\n",
    "#df_cl2_train = df_cl2_train[:,:-1]\n",
    "#df_cl2_test = df_cl2_test[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\dataX_1\\lib\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Lenovo\\.conda\\envs\\dataX_1\\lib\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-393-387239dc50dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf_cl2_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_cl2_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cl2_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_cl2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-393-387239dc50dc>\u001b[0m in \u001b[0;36mcorrelation\u001b[1;34m(dataset, test_dataset, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol_corr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[0mcolname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# getting the name of column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mcol_corr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dataX_1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1416\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dataX_1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   2092\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2093\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2094\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2095\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2096\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dataX_1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\dataX_1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2136\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def correlation(dataset, test_dataset, threshold=0.9):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    #corr_matrix = dataset.iloc[:,:-1].corr() #ignoring the target column\n",
    "\n",
    "    for i in range(len(dataset.columns)-1):\n",
    "        for j in range(i):\n",
    "            if (dataset.columns[j] not in col_corr) and (abs(np.corrcoef(dataset.iloc[:,i],dataset.iloc[:,j])[0][1]) >= threshold):\n",
    "                colname = dataset.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset\n",
    "                    del test_dataset[colname]\n",
    "\n",
    "    return dataset,test_dataset\n",
    "\n",
    "df_cl2_train,df_cl2_test = correlation(df_cl2_train,df_cl2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vars = VarianceThreshold()\n",
    "df_cl2_train = vars.fit_transform(df_cl2_train.values)\n",
    "df_cl2_test = vars.transform(df_cl2_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#536 features, 1 label\n",
    "\n",
    "scaling_transf = MinMaxScaler()\n",
    "X_train = scaling_transf.fit_transform(df_cl2_train[:,:-1])\n",
    "X_test = scaling_transf.transform(df_cl2_test[:,:-1])\n",
    "\n",
    "y_train = df_cl2_train[:,-1]\n",
    "y_test = df_cl2_test[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures=SelectKBest(f_regression, k=1000)\n",
    "X_train = bestFeatures.fit_transform(X_train, y_train)\n",
    "X_test = bestFeatures.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   54.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=False, criterion='mae', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=1,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rfRegressor = RandomForestRegressor(n_estimators=100,criterion=\"mae\",min_samples_split=2,min_samples_leaf=1,bootstrap=False,n_jobs=-1,verbose=1) #\n",
    "rfRegressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8808333367322163"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfRegressor.predict(X_test)\n",
    "mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfRegressor.predict(X_train)\n",
    "mean_absolute_error(y_train,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['immunome.csv',\n",
       " 'SerumLuminex.csv',\n",
       " 'plasmaLuminex.csv',\n",
       " 'plasmaSomalogic.csv',\n",
       " 'cfRNA.csv',\n",
       " 'metabolome.csv',\n",
       " 'microbiome.csv']"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_cl3_file_names = sub_cl1_file_names+sub_cl2_file_names\n",
    "sub_cl3_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cl3 = pd.concat([df_cfRNA,df_metabolome.iloc[:,1:],df_microbiome.iloc[:,1:],df_immunome.iloc[:,1:],df_SerumLuminex.iloc[:,1:],df_plasmaLuminex.iloc[:,1:],df_plasmaSomalogic.iloc[:,1:]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl3 =pd.merge(df_cl3,targets_df,on=\"SampleID\",).iloc[:,1:] #.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl3_train = df_cl3[df_cl3.iloc[:,-1]==\"train\"].iloc[:,:-1] \n",
    "df_cl3_test = df_cl3[df_cl3.iloc[:,-1]==\"test\"].iloc[:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vars = VarianceThreshold()\n",
    "df_cl3_train = vars.fit_transform(df_cl3_train.values)\n",
    "df_cl3_test = vars.transform(df_cl3_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "#536 features, 1 label\n",
    "\n",
    "scaling_transf = MinMaxScaler()\n",
    "X_train = scaling_transf.fit_transform(df_cl3_train[:,:-1])\n",
    "X_test = scaling_transf.transform(df_cl3_test[:,:-1])\n",
    "\n",
    "y_train = df_cl3_train[:,-1]\n",
    "y_test = df_cl3_test[:,-1]\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestFeatures=SelectKBest(f_regression, k=1000)\n",
    "X_train = bestFeatures.fit_transform(X_train, y_train)\n",
    "X_test = bestFeatures.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    6.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=False, criterion='mae', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "                      oob_score=False, random_state=None, verbose=1,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rfRegressor = RandomForestRegressor(n_estimators=100,criterion=\"mae\",min_samples_split=2,min_samples_leaf=1,bootstrap=False,n_jobs=-1,verbose=1) #\n",
    "rfRegressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4716666648714318"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfRegressor.predict(X_test)\n",
    "mean_absolute_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rfRegressor.predict(X_train)\n",
    "mean_absolute_error(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
